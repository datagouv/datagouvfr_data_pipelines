{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from git import Repo, Git\n",
    "import os\n",
    "import shutil\n",
    "from table_schema_to_markdown import convert_source\n",
    "import frictionless\n",
    "import glob\n",
    "import json\n",
    "import jsonschema\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import codecs\n",
    "import requests\n",
    "from urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FOLDER = TMP_FOLDER+'cache'\n",
    "DATA_FOLDER1 = TMP_FOLDER+'data'\n",
    "DATA_FOLDER2 = TMP_FOLDER+'data2'\n",
    "ERRORS_REPORT = []\n",
    "SCHEMA_INFOS = {}\n",
    "SCHEMA_CATALOG = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze validity of every release of every schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading yaml file containing all schemas that we want to display in schema.data.gouv.fr\n",
    "r = requests.get(LIST_SCHEMAS_YAML)\n",
    "\n",
    "with open(TMP_FOLDER+'repertoires.yml', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "with open(TMP_FOLDER+'repertoires.yml', \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_create_folder(folder):\n",
    "    \"\"\"Remove local folder if exist and (re)create it\"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consolidated_version(tag):\n",
    "    \"\"\"Analyze tag from a code source release, cast it to acceptable semver version X.X.X\"\"\"\n",
    "    valid_version = True\n",
    "    # Removing 'v' or 'V' from tag\n",
    "    version_items = str(tag).replace('v','').replace('V','').split('.')\n",
    "    # Add a patch number if only 2 items\n",
    "    if len(version_items) == 2: version_items.append('0')\n",
    "    # If more than 3, do not accept tag\n",
    "    if len(version_items) > 3: valid_version = False\n",
    "    # Verify if all items are digits\n",
    "    for v in version_items:\n",
    "        if not v.isdigit():\n",
    "            valid_version = False\n",
    "    # Return semver version and validity of it\n",
    "    return '.'.join(version_items), valid_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_errors(repertoire_slug, version, reason):\n",
    "    \"\"\"Create dictionnary that will populate ERRORS_REPORT object\"\"\"\n",
    "    errors = {}\n",
    "    errors['schema'] = repertoire_slug\n",
    "    errors['version'] = version\n",
    "    errors['type'] = reason\n",
    "    ERRORS_REPORT.append(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(repertoire_slug, conf, schema_type):\n",
    "    \"\"\"Check validity of schema and all of its releases\"\"\"\n",
    "    # schema_name in schema.data.gouv.fr is referenced by group and repo name in Git. Ex : etalab / schema-irve-statique\n",
    "    schema_name = '/'.join(conf['url'].split('.git')[0].split('/')[-2:])\n",
    "    # define source folder and create it\n",
    "    # source folder will help us to checkout to every release and analyze source code for each one\n",
    "    src_folder = CACHE_FOLDER + '/' + schema_name + '/'\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # clone repo in source folder\n",
    "    Repo.clone_from(conf['url'], src_folder)\n",
    "    repo = Repo(src_folder)\n",
    "    # get tags of repo\n",
    "    tags = sorted(repo.tags, key=lambda t: t.commit.committed_datetime)\n",
    "    \n",
    "    list_schemas = {}\n",
    "    \n",
    "    # Defining SCHEMA_INFOS object for website use\n",
    "    SCHEMA_INFOS[schema_name] = {}\n",
    "    SCHEMA_INFOS[schema_name]['homepage'] = conf['url']\n",
    "    SCHEMA_INFOS[schema_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['type'] = conf['type']\n",
    "    SCHEMA_INFOS[schema_name]['email'] = conf['email']\n",
    "    SCHEMA_INFOS[schema_name]['labels'] = conf['labels'] if 'labels' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    SCHEMA_INFOS[schema_name]['versions'] = {}\n",
    "    \n",
    "    # for every tags\n",
    "    for t in tags:\n",
    "        # get semver version and validity of it\n",
    "        version, valid_version = get_consolidated_version(t)\n",
    "        # if semver ok\n",
    "        if(valid_version):\n",
    "            # define destination folder and create it\n",
    "            # destination folder will store pertinents files for website for each version of each schema\n",
    "            dest_folder = DATA_FOLDER1 + '/' + schema_name + '/' + version + '/'\n",
    "            os.makedirs(dest_folder, exist_ok=True)\n",
    "            # checkout to current version\n",
    "            g = Git(src_folder)\n",
    "            g.checkout(str(t))\n",
    "            \n",
    "            conf_schema = None\n",
    "            # Managing validation differently for each type of schema\n",
    "            # tableschema will use frictionless package\n",
    "            # jsonschema will use jsonschema package\n",
    "            # other will only check if schema.yml file is present and contain correct information\n",
    "            if(schema_type == 'tableschema'):\n",
    "                list_schemas = manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'jsonschema'):\n",
    "                list_schemas, conf_schema = manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "            if(schema_type == 'other'):\n",
    "                list_schemas, conf_schema = manage_other(src_folder, dest_folder, list_schemas, version, schema_name)\n",
    "          \n",
    "    # Find latest valid version and create a specific folder 'latest' copying files in it (for website use)\n",
    "    latest_folder, sf = manage_latest_folder(conf, schema_name)\n",
    "    # Indicate in schema_info object name of latest schema\n",
    "    SCHEMA_INFOS[schema_name]['latest'] = sf\n",
    "    schema_file = list_schemas[sf]\n",
    "    # Complete catalog with all relevant information of schema in it\n",
    "    schema_to_add_to_catalog = generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, conf_schema)\n",
    "    return schema_to_add_to_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_datapackage(repertoire_slug, conf, schema_type):\n",
    "    \"\"\"Check validity of schemas from a datapackage repo for all of its releases\"\"\"\n",
    "    # schema_name in schema.data.gouv.fr is referenced by group and repo name in Git. Ex : etalab / schema-irve-statique\n",
    "    \n",
    "    # define source folder and create it\n",
    "    # source folder will help us to checkout to every release and analyze source code for each one\n",
    "    dpkg_name = '/'.join(conf['url'].split('.git')[0].split('/')[-2:])\n",
    "    src_folder = CACHE_FOLDER + '/' + '/'.join(conf['url'].split('.git')[0].split('/')[-2:]) + '/'\n",
    "    os.makedirs(src_folder, exist_ok=True)\n",
    "    # clone repo in source folder\n",
    "    Repo.clone_from(conf['url'], src_folder)\n",
    "    repo = Repo(src_folder)\n",
    "    # get tags of repo\n",
    "    tags = sorted(repo.tags, key=lambda t: t.commit.committed_datetime)\n",
    "    \n",
    "    list_schemas = {}\n",
    "    schemas_to_add_to_catalog = []\n",
    "    \n",
    "    \n",
    "    # Defining SCHEMA_INFOS object for website use\n",
    "    SCHEMA_INFOS[dpkg_name] = {}\n",
    "    SCHEMA_INFOS[dpkg_name]['homepage'] = conf['url']\n",
    "    SCHEMA_INFOS[dpkg_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    SCHEMA_INFOS[dpkg_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    SCHEMA_INFOS[dpkg_name]['type'] = conf['type']\n",
    "    SCHEMA_INFOS[dpkg_name]['email'] = conf['email']\n",
    "    SCHEMA_INFOS[dpkg_name]['labels'] = conf['labels'] if 'labels' in conf else None\n",
    "    SCHEMA_INFOS[dpkg_name]['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    SCHEMA_INFOS[dpkg_name]['versions'] = {}\n",
    "    SCHEMA_INFOS[dpkg_name]['schemas'] = []\n",
    "\n",
    "    for t in tags:\n",
    "        # get semver version and validity of it\n",
    "        version, valid_version = get_consolidated_version(t)\n",
    "        # if semver ok\n",
    "        if(valid_version):\n",
    "            g = Git(src_folder)\n",
    "            g.checkout(str(t))\n",
    "\n",
    "            SCHEMA_INFOS[dpkg_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[dpkg_name]['versions'][version]['pages'] = []\n",
    "            \n",
    "            dest_folder = DATA_FOLDER1 + '/' + dpkg_name + '/' + version + '/'\n",
    "            os.makedirs(dest_folder, exist_ok=True)\n",
    "            for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', 'datapackage.json']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f, dest_folder + f)\n",
    "                    if f != 'datapackage.json':\n",
    "                        SCHEMA_INFOS[dpkg_name]['versions'][version]['pages'].append(f)\n",
    "                    else:\n",
    "                        SCHEMA_INFOS[dpkg_name]['versions'][version]['schema_url'] = '/' + dpkg_name + '/' + version + '/datapackage.json'\n",
    "\n",
    "            # Verify that a file datapackage.json is present\n",
    "            if(os.path.isfile(src_folder + 'datapackage.json')):\n",
    "                # Validate it with frictionless package\n",
    "                frictionless_report = frictionless.validate(src_folder + 'datapackage.json')\n",
    "                # If datapackage release is valid, then\n",
    "                if(frictionless_report['valid'] == True):\n",
    "                    with open(src_folder + 'datapackage.json') as out:\n",
    "                        dp = json.load(out)\n",
    "                             \n",
    "                    schemas_dp = [r['schema'] for r in dp['resources'] if 'schema' in r]\n",
    "                    \n",
    "                    for schema in schemas_dp:\n",
    "\n",
    "                        with open(src_folder + schema) as out:\n",
    "                            schema_json = json.load(out)\n",
    "                        \n",
    "                        schema_name = dpkg_name.split('/')[0] + '/' + schema_json['name']\n",
    "                        \n",
    "                        if schema_name not in list_schemas:\n",
    "                            list_schemas[schema_name] = {}\n",
    "                        \n",
    "                        list_schemas[schema_name][version] = schema.split('/')[-1]\n",
    "\n",
    "                        if schema_name not in SCHEMA_INFOS:\n",
    "                            # Defining SCHEMA_INFOS object for website use\n",
    "                            SCHEMA_INFOS[schema_name] = {}\n",
    "                            SCHEMA_INFOS[schema_name]['homepage'] = conf['url']\n",
    "                            SCHEMA_INFOS[schema_name]['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "                            SCHEMA_INFOS[schema_name]['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "                            SCHEMA_INFOS[schema_name]['type'] = 'tableschema'\n",
    "                            SCHEMA_INFOS[schema_name]['email'] = conf['email']\n",
    "                            SCHEMA_INFOS[schema_name]['versions'] = {}\n",
    "                            SCHEMA_INFOS[schema_name]['datapackage'] = dp['title']\n",
    "                            SCHEMA_INFOS[schema_name]['datapackage_id'] = dp['name']\n",
    "                            SCHEMA_INFOS[schema_name]['labels'] = conf['labels'] if 'labels' in conf else None\n",
    "                            SCHEMA_INFOS[schema_name]['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "                        \n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "                        \n",
    "                        # define destination folder and create it\n",
    "                        # destination folder will store pertinents files for website for each version of each schema\n",
    "                    \n",
    "                        schema_dest_folder = DATA_FOLDER1 + '/' + schema_name + '/' + version + '/'\n",
    "                        if len(schema.split('/')) > 1:\n",
    "                            os.makedirs(schema_dest_folder, exist_ok=True)\n",
    "                        shutil.copyfile(src_folder + schema, schema_dest_folder + schema.split('/')[-1])\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + schema.split(\"/\")[-1]\n",
    "                        for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md']:\n",
    "                            if(os.path.isfile(src_folder + '/'.join(schema.split('/')[:-1]) + '/' + f)):\n",
    "                                shutil.copyfile(src_folder + '/'.join(schema.split('/')[:-1]) + '/' + f, schema_dest_folder + f)\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "\n",
    "                        # Create documentation file and save it\n",
    "                        with open(schema_dest_folder + '/' + 'documentation.md', \"w\") as out:\n",
    "                            try:\n",
    "                                # From schema.json, we use tableschema_to_markdown package to convert it in a\n",
    "                                # readable mardown file that will be use for documentation\n",
    "                                convert_source(schema_dest_folder + schema.split('/')[-1], out, 'page',[])\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append('documentation.md')\n",
    "                            except:\n",
    "                                # if conversion is on error, we add it to ERRORS_REPORT\n",
    "                                manage_errors(repertoire_slug, version, 'convert to markdown')\n",
    "                        \n",
    "                        latest_folder, sf = manage_latest_folder(conf, schema_name)      \n",
    "                else:\n",
    "                    print('not valid')\n",
    "            else:\n",
    "                print('no datapackage')\n",
    "    # Find latest valid version and create a specific folder 'latest' copying files in it (for website use)\n",
    "    latest_folder, sf = manage_latest_folder(conf, dpkg_name)\n",
    "\n",
    "    for schema in schemas_dp:\n",
    "        with open(src_folder + schema) as out:\n",
    "            schema_json = json.load(out)\n",
    "        # Complete catalog with all relevant information of schema in it\n",
    "        statc = generate_catalog_object(\n",
    "            DATA_FOLDER1 + '/' + dpkg_name.split('/')[0] + '/' + schema_json['name'] + '/latest/',\n",
    "            list_schemas[dpkg_name.split('/')[0] + '/' + schema_json['name']],\n",
    "            schema.split('/')[-1],\n",
    "            'tableschema',\n",
    "            dpkg_name,\n",
    "            None,\n",
    "            dp\n",
    "        )\n",
    "        schemas_to_add_to_catalog.append(statc)\n",
    "        SCHEMA_INFOS[dpkg_name.split('/')[0] + '/' + schema_json['name']]['latest'] = sf\n",
    "        SCHEMA_INFOS[dpkg_name]['schemas'].append(dpkg_name.split('/')[0] + '/' + schema_json['name'])\n",
    "\n",
    "    SCHEMA_INFOS[dpkg_name]['latest'] = sf\n",
    "                \n",
    "\n",
    "    statc = generate_catalog_datapackage(latest_folder, dpkg_name, conf, list_schemas[dpkg_name.split('/')[0] + '/' + schema_json['name']])\n",
    "    schemas_to_add_to_catalog.append(statc)\n",
    "    \n",
    "    # schemas_to_add_to_catalog.append(schema_to_add_to_catalog)\n",
    "    return schemas_to_add_to_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_tableschema(src_folder, dest_folder, list_schemas, version, schema_name, schema_file='schema.json'):\n",
    "    \"\"\"Check validity of a schema release from tableschema type\"\"\"\n",
    "    # Verify that a file schema.json is present\n",
    "    if(os.path.isfile(src_folder + schema_file)):\n",
    "        # Validate it with frictionless package\n",
    "        frictionless_report = frictionless.validate_schema(src_folder + schema_file)\n",
    "        # If schema release is valid, then        \n",
    "        if(frictionless_report['valid'] == True):\n",
    "            list_schemas[version] = schema_file\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            subfolder = '/'.join(schema_file.split('/')[:-1]) + '/'\n",
    "            if subfolder == '/':\n",
    "                subfolder = ''\n",
    "            else:\n",
    "                os.makedirs(dest_folder + subfolder, exist_ok=True)\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in [schema_file, 'README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md']:\n",
    "                if(os.path.isfile(src_folder + subfolder + f)):\n",
    "                    shutil.copyfile(src_folder + subfolder + f, dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == schema_file):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + schema_file\n",
    "            # Create documentation file and save it\n",
    "            with open(dest_folder + 'documentation.md', \"w\") as out:\n",
    "                try:\n",
    "                    # From schema.json, we use tableschema_to_markdown package to convert it in a\n",
    "                    # readable mardown file that will be use for documentation\n",
    "                    convert_source(dest_folder + schema_file, out, 'page',[])\n",
    "                    SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append('documentation.md')\n",
    "                except:\n",
    "                    # if conversion is on error, we add it to ERRORS_REPORT\n",
    "                    manage_errors(repertoire_slug, version, 'convert to markdown')\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        else:\n",
    "            manage_errors(repertoire_slug, version, 'tableschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.json, schema release is not valid, we remove it from DATA_FOLDER1\n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing ' + schema_file)\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_jsonschema(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from jsonschema type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schemas.yml is present\n",
    "    # This file will indicate title, description of jsonschema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    # This file will also indicate which file store the jsonschema schema\n",
    "    if(os.path.isfile(src_folder + 'schemas.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schemas.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "                if('schemas' in conf_schema):\n",
    "                    s = conf_schema['schemas'][0]\n",
    "                    # Verify if jsonschema file indicate in schemas.yml is present, then load it\n",
    "                    if(os.path.isfile(src_folder + s['path'])):\n",
    "                        with open(src_folder + s['path'], \"r\") as f:\n",
    "                            schema_data = json.load(f)\n",
    "                        # Validate schema with jsonschema package\n",
    "                        jsonschema.validators.validator_for(schema_data).check_schema(schema_data)\n",
    "                        list_schemas[version] = s['path']\n",
    "                        # We complete info of version\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "                        # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "                        for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', s['path']]:\n",
    "                            if(os.path.isfile(src_folder + f)):\n",
    "                                os.makedirs(os.path.dirname(dest_folder + f), exist_ok=True)\n",
    "                                shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                            # if it is a markdown file, we will read them as page in website\n",
    "                            if(f[-3:] == '.md'):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                            # if it is the schema, we indicate it as it in object\n",
    "                            if(f == s['path']):\n",
    "                                SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + s['path']\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'jsonschema validation')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schemas.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schemas.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_other(src_folder, dest_folder, list_schemas, version, schema_name):\n",
    "    \"\"\"Check validity of a schema release from other type\"\"\"\n",
    "    conf_schema = None\n",
    "    # Verify that a file schema.yml is present\n",
    "    # This file will indicate title, description of schema (it is a prerequisite asked by schema.data.gouv.fr)\n",
    "    if(os.path.isfile(src_folder + 'schema.yml')):\n",
    "        try:\n",
    "            with open(src_folder + 'schema.yml', \"r\") as f:\n",
    "                conf_schema = yaml.safe_load(f)\n",
    "            list_schemas[version] = 'schema.yml'\n",
    "            # We complete info of version\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version] = {}\n",
    "            SCHEMA_INFOS[schema_name]['versions'][version]['pages'] = []\n",
    "            # We check for list of normalized files if it is present in source code, if so, we copy paste them into dest folder\n",
    "            for f in ['README.md', 'SEE_ALSO.md', 'CHANGELOG.md', 'CONTEXT.md', 'schema.yml']:\n",
    "                if(os.path.isfile(src_folder + f)):\n",
    "                    shutil.copyfile(src_folder + f,dest_folder + f)\n",
    "                    # if it is a markdown file, we will read them as page in website\n",
    "                    if(f[-3:] == '.md'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['pages'].append(f)\n",
    "                    # if it is the schema, we indicate it as it in object\n",
    "                    if(f == 'schema.yml'):\n",
    "                        SCHEMA_INFOS[schema_name]['versions'][version]['schema_url'] = '/' + schema_name + '/' + version + '/' + 'schema.yml'\n",
    "        # If schema release is not valid, we remove it from DATA_FOLDER1\n",
    "        except:\n",
    "            manage_errors(repertoire_slug, version, 'validation of type other')\n",
    "            shutil.rmtree(dest_folder)\n",
    "    # If there is no schema.yml, schema release is not valid, we remove it from DATA_FOLDER1    \n",
    "    else:\n",
    "        manage_errors(repertoire_slug, version, 'missing schema.yml')\n",
    "        shutil.rmtree(dest_folder)\n",
    "    \n",
    "    return list_schemas, conf_schema\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparer_versions(version):\n",
    "    return [int(part) for part in version.split('.')]\n",
    "\n",
    "def manage_latest_folder(conf, schema_name):\n",
    "    \"\"\"Create latest folder containing all files from latest valid version of a schema\"\"\"\n",
    "    # Get all valid version from a schema by analyzing folders, then sort them to get latest valid version and related folder\n",
    "    subfolders = [ f.name for f in os.scandir(DATA_FOLDER1 + '/' + schema_name + '/') if f.is_dir() ]\n",
    "    subfolders = sorted(subfolders, key=comparer_versions)\n",
    "    print(subfolders)\n",
    "    sf = subfolders[-1]\n",
    "    if sf == 'latest':\n",
    "        sf = subfolders[-2]\n",
    "    latest_version_folder = DATA_FOLDER1 + '/' + schema_name + '/' + sf + '/'\n",
    "    # Determine latest folder path then mkdir it\n",
    "    latest_folder = DATA_FOLDER1 + '/' + schema_name + '/latest/'\n",
    "    os.makedirs(latest_folder, exist_ok=True)\n",
    "    # For every file in latest valid version folder, copy them into \n",
    "    shutil.copytree(latest_version_folder, latest_folder, dirs_exist_ok=True)\n",
    "    # For website need, copy paste latest README into root of schema folder\n",
    "    shutil.copyfile(latest_version_folder+'README.md','/'.join(latest_version_folder.split('/')[:-2])+'/README.md')\n",
    "\n",
    "    return latest_folder, sf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_catalog_datapackage(latest_folder, dpkg_name, conf, list_schemas):\n",
    "    with open(latest_folder + 'datapackage.json', \"r\") as f:\n",
    "        dpkg = json.load(f)\n",
    "    mydict = {}\n",
    "    mydict['name'] = dpkg_name\n",
    "    mydict['title'] = dpkg['title']\n",
    "    mydict['description'] = dpkg['description']\n",
    "    mydict['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + dpkg_name + '/latest/' + 'datapackage.json'\n",
    "    mydict['schema_type'] = 'datapackage'\n",
    "    mydict['contact'] = conf['email']\n",
    "    mydict['examples'] = []\n",
    "    mydict['labels'] = conf['labels'] if 'labels' in conf else []\n",
    "    mydict['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    mydict['versions'] = []\n",
    "    for sf in list_schemas:\n",
    "        mydict2 = {}\n",
    "        mydict2['version_name'] = sf\n",
    "        mydict2['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + dpkg_name + '/' + sf + '/' + 'datapackage.json'\n",
    "        mydict['versions'].append(mydict2)\n",
    "    # These four following property are not in catalog spec \n",
    "    mydict['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    mydict['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    mydict['homepage'] = conf['url']\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_catalog_object(latest_folder, list_schemas, schema_file, schema_type, schema_name, obj_info=None, datapackage=None):\n",
    "    \"\"\"Generate dictionnary containing all relevant information for catalog\"\"\"\n",
    "    # If tableschema, relevant information are directly into schema.json, \n",
    "    # if not, relevant info are in yaml files with are stored in obj_info variable\n",
    "    if(schema_type == 'tableschema'):\n",
    "        with open(latest_folder + schema_file, \"r\") as f:\n",
    "            schema = json.load(f)\n",
    "    else:\n",
    "        schema = obj_info\n",
    "    # Complete dictionnary with relevant info needed in catalog\n",
    "    mydict = {}\n",
    "    if datapackage:\n",
    "        mydict['name'] = latest_folder.replace(DATA_FOLDER1 + '/', '').replace('/latest/', '')\n",
    "    else:\n",
    "        mydict['name'] = schema_name\n",
    "    mydict['title'] = schema['title']    \n",
    "    mydict['description'] = schema['description']\n",
    "    mydict['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + mydict['name'] + '/latest/' + schema_file\n",
    "    mydict['schema_type'] = schema_type\n",
    "    mydict['contact'] = conf['email']\n",
    "    mydict['examples'] = schema['resources'] if 'resources' in schema else []\n",
    "    mydict['labels'] = conf['labels'] if 'labels' in conf else []\n",
    "    mydict['consolidation_dataset_id'] = conf['consolidation'] if 'consolidation' in conf else None\n",
    "    mydict['versions'] = []\n",
    "    for sf in list_schemas:\n",
    "        mydict2 = {}\n",
    "        mydict2['version_name'] = sf\n",
    "        mydict2['schema_url'] = 'https://schema.data.gouv.fr/schemas/' + mydict['name'] + '/' + sf + '/' + list_schemas[sf]\n",
    "        mydict['versions'].append(mydict2)\n",
    "    # These four following property are not in catalog spec \n",
    "    mydict['external_doc'] = conf['external_doc'] if 'external_doc' in conf else None\n",
    "    mydict['external_tool'] = conf['external_tool'] if 'external_tool' in conf else None\n",
    "    mydict['homepage'] = conf['url']\n",
    "    if(datapackage):\n",
    "        mydict['datapackage_title'] = datapackage['title']\n",
    "        mydict['datapackage_name'] = schema_name\n",
    "        mydict['datapackage_description'] = datapackage['description'] if 'description' in datapackage else None\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoffrey/.pyenv/versions/3.8.12/envs/analytics/lib/python3.8/site-packages/frictionless/actions/validate.py:333: UserWarning: Function \"validate_schema\" is deprecated (use \"schema.validate\").\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/geoffrey/.pyenv/versions/3.8.12/envs/analytics/lib/python3.8/site-packages/frictionless/actions/validate.py:333: UserWarning: Function \"validate_schema\" is deprecated (use \"schema.validate\").\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'etalab/schema-dispositif-aide', 'title': \"Dispositifs d'aides\", 'description': \"Spécification du fichier d'échange relatif aux dispositifs d'aides.\", 'schema_url': 'https://schema.data.gouv.fr/schemas/etalab/schema-dispositif-aide/latest/schema.json', 'schema_type': 'tableschema', 'contact': 'aides-territoires@beta.gouv.fr', 'examples': [{'title': 'Fichier valide (CSV)', 'name': 'exemple-valide-csv', 'path': 'https://github.com/etalab/schema-dispositif-aide/raw/v0.0.1/exemple-valide.csv'}], 'labels': [], 'consolidation_dataset_id': None, 'versions': [{'version_name': '0.0.2', 'schema_url': 'https://schema.data.gouv.fr/schemas/etalab/schema-dispositif-aide/0.0.2/schema.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/etalab/schema-dispositif-aide.git'}\n",
      "--- dispositif-aide processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geoffrey/.pyenv/versions/3.8.12/envs/analytics/lib/python3.8/site-packages/frictionless/actions/validate.py:207: UserWarning: Function \"validate_resource\" is deprecated (use \"resource.validate\").\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/geoffrey/.pyenv/versions/3.8.12/envs/analytics/lib/python3.8/site-packages/frictionless/actions/validate.py:207: UserWarning: Function \"validate_resource\" is deprecated (use \"resource.validate\").\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- irve processed\n",
      "[{'name': 'etalab/schema-dispositif-aide', 'title': \"Dispositifs d'aides\", 'description': \"Spécification du fichier d'échange relatif aux dispositifs d'aides.\", 'schema_url': 'https://schema.data.gouv.fr/schemas/etalab/schema-dispositif-aide/latest/schema.json', 'schema_type': 'tableschema', 'contact': 'aides-territoires@beta.gouv.fr', 'examples': [{'title': 'Fichier valide (CSV)', 'name': 'exemple-valide-csv', 'path': 'https://github.com/etalab/schema-dispositif-aide/raw/v0.0.1/exemple-valide.csv'}], 'labels': [], 'consolidation_dataset_id': None, 'versions': [{'version_name': '0.0.2', 'schema_url': 'https://schema.data.gouv.fr/schemas/etalab/schema-dispositif-aide/0.0.2/schema.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/etalab/schema-dispositif-aide.git'}, {'name': 'geoffreyaldebert/irve-dynamique', 'title': 'IRVE dynamique', 'description': \"Spécification du fichier d'échange relatif aux données concernant la localisation géographique et les caractéristiques techniques des stations et des points de recharge pour véhicules électriques\", 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/latest/schema-dynamique.json', 'schema_type': 'tableschema', 'contact': 'test@beta.gouv.fr', 'examples': [{'title': 'Exemple de fichier IRVE valide', 'path': 'https://github.com/etalab/schema-irve/raw/v2.0.4/exemple-valide-dynamique.csv'}], 'labels': [], 'consolidation_dataset_id': None, 'versions': [{'version_name': '2.0.3', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.3/schema-dynamique.json'}, {'version_name': '2.0.4', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.4/schema-dynamique.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/schema-irve.git', 'datapackage_title': 'Infrastructures de recharges pour véhicules électriques', 'datapackage_name': 'geoffreyaldebert/schema-irve', 'datapackage_description': \"data package d'exemple pour les IRVE (statique et dynamique)\"}, {'name': 'geoffreyaldebert/irve-statique', 'title': 'IRVE statique', 'description': \"Spécification du fichier d'échange relatif aux données concernant la localisation géographique et les caractéristiques techniques des stations et des points de recharge pour véhicules électriques\", 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/latest/schema-statique.json', 'schema_type': 'tableschema', 'contact': 'test@beta.gouv.fr', 'examples': [{'title': 'Exemple de fichier IRVE valide', 'path': 'https://github.com/etalab/schema-irve/raw/v2.0.4/exemple-valide-statique.csv'}], 'labels': [], 'consolidation_dataset_id': None, 'versions': [{'version_name': '2.0.3', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.3/schema-statique.json'}, {'version_name': '2.0.4', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.4/schema-statique.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/schema-irve.git', 'datapackage_title': 'Infrastructures de recharges pour véhicules électriques', 'datapackage_name': 'geoffreyaldebert/schema-irve', 'datapackage_description': \"data package d'exemple pour les IRVE (statique et dynamique)\"}, {'name': 'geoffreyaldebert/schema-irve', 'title': 'Infrastructures de recharges pour véhicules électriques', 'description': \"data package d'exemple pour les IRVE (statique et dynamique)\", 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/latest/datapackage.json', 'schema_type': 'datapackage', 'contact': 'test@beta.gouv.fr', 'examples': [], 'labels': [], 'consolidation_dataset_id': None, 'versions': [{'version_name': '2.0.3', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.3/datapackage.json'}, {'version_name': '2.0.4', 'schema_url': 'https://schema.data.gouv.fr/schemas/geoffreyaldebert/schema-irve/2.0.4/datapackage.json'}], 'external_doc': None, 'external_tool': None, 'homepage': 'https://github.com/geoffreyaldebert/schema-irve.git'}]\n"
     ]
    }
   ],
   "source": [
    "# Clean and (re)create CACHE AND DATA FOLDER\n",
    "clean_and_create_folder(CACHE_FOLDER)\n",
    "clean_and_create_folder(DATA_FOLDER1)\n",
    "\n",
    "# Initiate Catalog\n",
    "SCHEMA_CATALOG['$schema'] = 'https://opendataschema.frama.io/catalog/schema-catalog.json'\n",
    "SCHEMA_CATALOG['version'] = 1\n",
    "SCHEMA_CATALOG['schemas'] = []\n",
    "\n",
    "# For every schema in repertoires.yml, check it\n",
    "for repertoire_slug, conf in config.items():\n",
    "    try:\n",
    "        if(conf['type'] != 'datapackage'):\n",
    "            schema_to_add_to_catalog = check_schema(repertoire_slug, conf, conf['type'])\n",
    "            SCHEMA_CATALOG['schemas'].append(schema_to_add_to_catalog)\n",
    "        else:\n",
    "            schemas_to_add_to_catalog = check_datapackage(repertoire_slug, conf, conf['type'])\n",
    "            for schema in schemas_to_add_to_catalog:\n",
    "                SCHEMA_CATALOG['schemas'].append(schema)\n",
    "        # Append info to SCHEMA_CATALOG\n",
    "        print('--- {} processed'.format(repertoire_slug))\n",
    "    except:\n",
    "        print('--- {} failed to process'.format(repertoire_slug)) \n",
    "\n",
    "\n",
    "\n",
    "schemas_scdl = SCHEMA_CATALOG.copy()\n",
    "schemas_transport = SCHEMA_CATALOG.copy()\n",
    "schemas_tableschema = SCHEMA_CATALOG.copy()\n",
    "\n",
    "# Save catalog to schemas.json file\n",
    "with open(DATA_FOLDER1 + '/schemas.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_CATALOG, fp)\n",
    "\n",
    "schemas_scdl['schemas'] = [x for x in schemas_scdl['schemas'] if 'Socle Commun des Données Locales' in x['labels']]\n",
    "\n",
    "schemas_transport['schemas'] = [x for x in schemas_transport['schemas'] if 'transport.data.gouv.fr' in x['labels']]\n",
    "\n",
    "schemas_tableschema['schemas'] = [x for x in schemas_tableschema['schemas'] if x['schema_type'] == 'tableschema']\n",
    "\n",
    "with open(DATA_FOLDER1 + '/schemas-scdl.json', 'w') as fp:\n",
    "    json.dump(schemas_scdl, fp)\n",
    "\n",
    "with open(DATA_FOLDER1 + '/schemas-transport-data-gouv-fr.json', 'w') as fp:\n",
    "    json.dump(schemas_transport, fp)\n",
    "\n",
    "with open(DATA_FOLDER1 + '/schemas-tableschema.json', 'w') as fp:\n",
    "    json.dump(schemas_tableschema, fp)\n",
    "\n",
    "# Save schemas_infos to schema-infos.json file\n",
    "with open(DATA_FOLDER1 + '/schema-infos.json', 'w') as fp:\n",
    "    json.dump(SCHEMA_INFOS, fp)\n",
    "    \n",
    "# Save errors to errors.json file\n",
    "with open(DATA_FOLDER1 + '/errors.json', 'w') as fp:\n",
    "    json.dump(ERRORS_REPORT, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema relative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_links(md):\n",
    "    \"\"\"Returns dict of links in markdown:\n",
    "    'regular': [foo](some.url)\n",
    "    'footnotes': [foo][3]\n",
    "    \n",
    "    [3]: some.url\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/a/30738268/2755116\n",
    "    INLINE_LINK_RE = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "    FOOTNOTE_LINK_TEXT_RE = re.compile(r'\\[([^\\]]+)\\]\\[(\\d+)\\]')\n",
    "    FOOTNOTE_LINK_URL_RE = re.compile(r'\\[(\\d+)\\]:\\s+(\\S+)')\n",
    "\n",
    "    links = list(INLINE_LINK_RE.findall(md))\n",
    "    footnote_links = dict(FOOTNOTE_LINK_TEXT_RE.findall(md))\n",
    "    footnote_urls = dict(FOOTNOTE_LINK_URL_RE.findall(md))\n",
    "\n",
    "    footnotes_linking = []\n",
    "        \n",
    "    for key in footnote_links.keys():\n",
    "        footnotes_linking.append((footnote_links[key], footnote_urls[footnote_links[key]]))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLinksDocumentation(dest_folder):\n",
    "    \"\"\"Custom cleaning for links in markdown\"\"\"\n",
    "    # For every documentation.md file, do some custom cleaning for links\n",
    "    file = codecs.open(dest_folder + 'documentation.md', \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Find all links in file\n",
    "    links = find_md_links(data)\n",
    "    # For each one, lower string then manage space ; _ ; --- and replace them by -\n",
    "    for (name, link) in links:\n",
    "        if(link.startswith('#')):\n",
    "            newlink = link.lower()\n",
    "            newlink = newlink.replace(' ','-')\n",
    "            newlink = newlink.replace('_','-')\n",
    "            newlink = unidecode(newlink, \"utf-8\")\n",
    "            newlink = newlink.replace('---','-')\n",
    "            data = data.replace(link,newlink)\n",
    "    # Save modifications\n",
    "    with open(dest_folder + 'documentation.md', 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFrontToMarkdown(dest_folder, f):\n",
    "    \"\"\"Custom add to every markdown files\"\"\"\n",
    "    # for every markdown files\n",
    "    file = codecs.open(dest_folder + f, \"r\", \"utf-8\")\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Add specific tag for website interpretation\n",
    "    data = \"<MenuSchema />\\n\\n\"+data\n",
    "    # Exception scdl Budget not well interpreted by vuepress\n",
    "    data = data.replace('<DocumentBudgetaire>', 'DocumentBudgetaire')\n",
    "    # Save modification\n",
    "    with open(dest_folder + f, 'w', encoding='utf-8') as fin:\n",
    "        fin.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    \"\"\"Get list off all files in a specific folder\"\"\"\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all files in DATA_FOLDER\n",
    "files = []\n",
    "files = getListOfFiles(DATA_FOLDER1)\n",
    "# Create list of file that we do not want to copy paste\n",
    "avoid_files = [DATA_FOLDER1 + '/' + s['name'] + '/README.md' for s in SCHEMA_CATALOG['schemas']]\n",
    "# for every file\n",
    "for f in files:\n",
    "    # if it is a markdown, add custom front to content\n",
    "    if(f[-3:] == '.md'):\n",
    "        addFrontToMarkdown('/'.join(f.split('/')[:-1])+'/', f.split('/')[-1])\n",
    "    # if it is the documentation file, clean links on it\n",
    "    if(f.split('/')[-1] == 'documentation.md'):\n",
    "        cleanLinksDocumentation('/'.join(f.split('/')[:-1])+'/')\n",
    "    # if it is a README file (except if on avoid_list), copy paste it to root folder of schema (for website use)\n",
    "    # That will create README file with name X.X.X.md (X.X.X corresponding to a specific version)\n",
    "    if(f.split('/')[-1] == 'README.md'):\n",
    "        if f not in avoid_files:\n",
    "            shutil.copyfile(f, f.replace('/README.md','.md'))\n",
    "\n",
    "# Clean and (re)create DATA_FOLDER2, then copy paste all DATA_FOLDER1 into DATA_FOLDER2\n",
    "# DATA_FOLDER1 will be use to contain all markdown files\n",
    "# DATA_FOLDER2 will be use to contain all yaml and json files \n",
    "# This is needed for vuepress that need to store page in one place and 'resources' in another\n",
    "if os.path.exists(DATA_FOLDER2):\n",
    "    shutil.rmtree(DATA_FOLDER2)\n",
    "shutil.copytree(DATA_FOLDER1, DATA_FOLDER2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributors(url):\n",
    "    \"\"\"Get list off all contributors of a specific git repo\"\"\"\n",
    "    parse_url = parse.urlsplit(url)\n",
    "    # if github, use github api\n",
    "    if('github.com' in parse_url.netloc):\n",
    "        api_url =  parse_url.scheme+'://api.github.com/repos/'+parse_url.path[1:].replace('.git','')+'/contributors'\n",
    "    # else, use gitlab api\n",
    "    else:\n",
    "        api_url =  parse_url.scheme+'://'+parse_url.netloc+'/api/v4/projects/'+parse_url.path[1:].replace('/','%2F').replace('.git','')+'/repository/contributors'\n",
    "    try:\n",
    "        r = requests.get(api_url)\n",
    "        return len(r.json())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every issue, request them by label schema status (en investigation or en construction)\n",
    "mydict = {}\n",
    "labels = ['construction', 'investigation']\n",
    "# For each label, get relevant info via github api of schema.data.gouv.fr repo\n",
    "try:\n",
    "    for l in labels:\n",
    "        r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue&labels=Sch%C3%A9ma%20en%20'+l)\n",
    "        mydict[l] = []\n",
    "        for issue in r.json():\n",
    "            mydict2 = {}\n",
    "            mydict2['created_at'] = issue['created_at']\n",
    "            mydict2['labels'] = [l]\n",
    "            mydict2['nb_comments'] = issue['comments']\n",
    "            mydict2['title'] = issue['title']\n",
    "            mydict2['url'] = issue['html_url']\n",
    "            mydict[l].append(mydict2)\n",
    "\n",
    "    # Find number of current issue in schema.data.gouv.fr repo\n",
    "    r = requests.get('https://api.github.com/repos/etalab/schema.data.gouv.fr/issues?q=is%3Aopen+is%3Aissue')\n",
    "    mydict['nb_issues'] = len(r.json())\n",
    "\n",
    "    # for every schema, find relevant info in data.gouv.fr API\n",
    "    mydict['references'] = {}\n",
    "    for s in SCHEMA_CATALOG['schemas']:\n",
    "        r = requests.get('https://www.data.gouv.fr/api/1/datasets/?schema='+s['name'])\n",
    "        mydict['references'][s['name']] = {}\n",
    "        mydict['references'][s['name']]['dgv_resources'] = r.json()['total']\n",
    "        mydict['references'][s['name']]['title'] = s['title']\n",
    "        mydict['references'][s['name']]['contributors'] = get_contributors(s['homepage'])\n",
    "\n",
    "    # Save stats infos to stats.json file\n",
    "    with open(DATA_FOLDER2 + '/stats.json', 'w') as fp:\n",
    "        json.dump(mydict, fp)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prod folders for website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_files_extension(folder, extension):\n",
    "    \"\"\"Remove all file of a specific extension in a folder\"\"\"\n",
    "    files = []\n",
    "    files = getListOfFiles(folder)\n",
    "    for f in files:\n",
    "        if f[-1*len(extension):] == extension:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all markdown from DATA_FOLDER1 and all json, yaml and yml file of DATA_FOLDER2\n",
    "remove_all_files_extension(DATA_FOLDER2, '.md')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.json')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yml')\n",
    "remove_all_files_extension(DATA_FOLDER1, '.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6bdeb9600fc2cd3c94336bd4e6e4e04fad87a3049736fa20025d02b00c95294d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
