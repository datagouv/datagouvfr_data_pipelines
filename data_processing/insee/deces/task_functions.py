from datetime import datetime, timedelta
import logging
import json
import os
import re

import requests
import pandas as pd

from datagouvfr_data_pipelines.config import (
    AIRFLOW_ENV,
    AIRFLOW_DAG_HOME,
    AIRFLOW_DAG_TMP,
    MINIO_BUCKET_DATA_PIPELINE_OPEN,
)
from datagouvfr_data_pipelines.utils.conversions import csv_to_parquet
from datagouvfr_data_pipelines.utils.filesystem import File
from datagouvfr_data_pipelines.utils.s3 import S3Client
from datagouvfr_data_pipelines.utils.datagouv import (
    local_client,
)
from datagouvfr_data_pipelines.utils.mattermost import send_message
from datagouvfr_data_pipelines.utils.utils import MOIS_FR

DAG_FOLDER = "datagouvfr_data_pipelines/data_processing/"
DATADIR = f"{AIRFLOW_DAG_TMP}deces"
s3_open = S3Client(bucket=MINIO_BUCKET_DATA_PIPELINE_OPEN)
with open(f"{AIRFLOW_DAG_HOME}{DAG_FOLDER}insee/deces/config/dgv.json") as fp:
    config = json.load(fp)


def check_if_modif():
    return local_client.resource(
        id=config["deces_csv"][AIRFLOW_ENV]["resource_id"],
    ).check_if_more_recent_update(dataset_id="5de8f397634f4164071119c5")


def clean_period(file_name):
    return file_name.replace("deces-", "").replace(".txt", "")


def build_temporal_coverage(min_date, max_date):
    # min_date is just a year
    min_iso = f"{min_date}-01-01T00:00:00.000000Z"
    # max_date looks like YYYY-mMM, we're setting the end to the end of the month
    tmp_max = datetime.strptime(f"{max_date.replace('m', '')}-01", "%Y-%m-%d")
    tmp_max = (tmp_max.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(
        days=1
    )
    max_iso = f"{tmp_max.strftime('%Y-%m-%d')}T00:00:00.000000Z"
    return min_iso, max_iso


def build_year_month(period):
    if "m" not in period:
        return period
    year, month = period.split("-")
    month = MOIS_FR[month.replace("m", "")]
    return f"{month} {year}"


def reshape_date(date_no_dash):
    year = date_no_dash[:4]
    month = date_no_dash[4:6]
    day = date_no_dash[6:8]
    return f"{year}-{month}-{day}"


def get_fields(row):
    nom_prenom = row[:80].strip()
    d = {
        "nom": nom_prenom.split("*")[0],
        "prenoms": nom_prenom.split("*")[1].replace("/", "").replace(" ", ","),
        "sexe": row[80].replace("1", "M").replace("2", "F"),
        "date_naissance": row[81:89],
        "code_insee_naissance": row[89:94],
        "commune_naissance": row[94:124].strip(),
        # quite some issues in the countries, maybe a cleaning func?
        # or do we want to stick to the original?
        "pays_naissance": (
            row[124:154].strip() or "FRANCE OU ANCIENS TERRITOIRES FRANCAIS"
        ),
        "date_deces": row[154:162],
        "code_insee_deces": row[162:167],
        "numero_acte_deces": row[167:176].strip(),
    }
    return d


def gather_data(ti):
    logging.info("Getting resources list")
    resources = requests.get(
        "https://www.data.gouv.fr/api/1/datasets/5de8f397634f4164071119c5/",
        headers={"X-fields": "resources{url,title}"},
    ).json()["resources"]
    year_regex = r"deces-\d{4}.txt"
    month_regex = r"deces-\d{4}-m\d{2}.txt"
    full_years = []
    urls = {}
    for r in resources:
        if re.match(year_regex, r["title"]):
            urls[clean_period(r["title"])] = r["url"]
            full_years.append(r["title"][6:10])
    logging.info(full_years)
    for r in resources:
        if re.match(month_regex, r["title"]) and r["title"][6:10] not in full_years:
            logging.info(r["title"])
            urls[clean_period(r["title"])] = r["url"]

    opposition_url = [r["url"] for r in resources if "opposition" in r["title"]]
    if len(opposition_url) != 1:
        raise ValueError(
            f"There should be exactly one opposition file, {len(opposition_url)} found"
        )
    df_opposition = pd.read_csv(
        opposition_url[0],
        sep=";",
        dtype=str,
    )
    df_opposition.rename(
        {
            "Date de décès": "date_deces",
            "Code du lieu de décès": "code_insee_deces",
            "Numéro d'acte de décès": "numero_acte_deces",
        },
        axis=1,
        inplace=True,
    )
    df_opposition["opposition"] = True

    errors = []
    for idx, (origin, rurl) in enumerate(urls.items()):
        data = []
        logging.info(f"Proccessing {origin}")
        rows = requests.get(rurl).text.split("\n")
        for r in rows:
            if not r:
                continue
            try:
                fields = get_fields(r)
                data.append({**fields, "fichier_origine": origin})
            except Exception:
                logging.warning(r)
                errors.append(r)
        # can't have the whole dataframe in RAM, so saving in batches
        df = pd.merge(
            pd.DataFrame(data),
            df_opposition,
            how="left",
            on=["date_deces", "code_insee_deces", "numero_acte_deces"],
        )
        df["opposition"] = df["opposition"].fillna(False)
        del data
        df.to_csv(
            DATADIR + "/deces.csv",
            index=False,
            mode="w" if idx == 0 else "a",
            header=idx == 0,
        )
        del df
    logging.warning(f"> {len(errors)} erreur(s)")
    # conversion to parquet, opposition is a boolean, dates should be dates but
    # partially missing ones are uncastable (e.g 1950-00-12)
    dtype = {
        "nom": "VARCHAR",
        "prenoms": "VARCHAR",
        "sexe": "VARCHAR",
        "date_naissance": "VARCHAR",
        "code_insee_naissance": "VARCHAR",
        "commune_naissance": "VARCHAR",
        "pays_naissance": "VARCHAR",
        "date_deces": "VARCHAR",
        "code_insee_deces": "VARCHAR",
        "numero_acte_deces": "VARCHAR",
        "fichier_origine": "VARCHAR",
        "opposition": "BOOLEAN",
    }
    csv_to_parquet(
        DATADIR + "/deces.csv",
        sep=",",
        dtype=dtype,
    )

    ti.xcom_push(key="min_date", value=min(urls.keys()))
    ti.xcom_push(
        key="max_date",
        # in January we have only full year files so we manually add that it goes until December of the previous year
        value=(m if "-" in (m := max(urls.keys())) else m + "-m12"),
    )


def send_to_s3():
    s3_open.send_files(
        list_files=[
            File(
                source_path=f"{DATADIR}/",
                source_name=f"deces.{_ext}",
                dest_path="deces/",
                dest_name=f"deces.{_ext}",
            )
            for _ext in ["csv", "parquet"]
        ],
        ignore_airflow_env=True,
    )


def publish_on_datagouv(ti):
    min_date = ti.xcom_pull(key="min_date", task_ids="gather_data")
    max_date = ti.xcom_pull(key="max_date", task_ids="gather_data")
    for _ext in ["csv", "parquet"]:
        local_client.resource(
            id=config[f"deces_{_ext}"][AIRFLOW_ENV]["resource_id"],
            dataset_id=config[f"deces_{_ext}"][AIRFLOW_ENV]["dataset_id"],
            fetch=False,
        ).update(
            payload={
                "url": (
                    f"https://object.files.data.gouv.fr/{MINIO_BUCKET_DATA_PIPELINE_OPEN}"
                    f"/deces/deces.{_ext}"
                ),
                "filesize": os.path.getsize(DATADIR + f"/deces.{_ext}"),
                "title": (
                    f"Personnes décédées entre {min_date} et {build_year_month(max_date)} (format {_ext})"
                ),
                "format": _ext,
                "description": (
                    f"Personnes décédées entre {min_date} et {build_year_month(max_date)} (format {_ext})"
                    " (créé à partir des [fichiers de l'INSEE]"
                    "(https://www.data.gouv.fr/datasets/5de8f397634f4164071119c5/))"
                ),
            },
        )
    min_iso, max_iso = build_temporal_coverage(min_date, max_date)
    local_client.dataset(
        config["deces_csv"][AIRFLOW_ENV]["dataset_id"], fetch=False
    ).update(
        payload={
            "temporal_coverage": {
                "start": min_iso,
                "end": max_iso,
            },
        },
    )


def notification_mattermost():
    dataset_id = config["deces_csv"][AIRFLOW_ENV]["dataset_id"]
    send_message(
        f"Données décès agrégées :"
        f"\n- uploadées sur Minio"
        f"\n- publiées [sur {'demo.' if AIRFLOW_ENV == 'dev' else ''}data.gouv.fr]"
        f"({local_client.base_url}/datasets/{dataset_id}/)"
    )
